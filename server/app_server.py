from fastapi import FastAPI, HTTPException, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from graph.conversation_graph import ConversationGraph
from utils.config_ import load_config
from utils.error_handler import error_handler
from utils.monitoring import monitoring
from supabase import create_client, Client
from utils.vectorstore import initialize_vectorstore, decrypt_vectorstore, load_vectorstore
import os, traceback
from dotenv import load_dotenv
from datetime import datetime
from fastapi.responses import JSONResponse, FileResponse, HTMLResponse
from fastapi.encoders import jsonable_encoder
from models.chat_models import ChatRequest
from fastapi.staticfiles import StaticFiles
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings  # ìˆ˜ì •ëœ ì„í¬íŠ¸
import json
import faiss
import numpy as np
from openai import OpenAI
from utils.vectorstore import decrypt_vectorstore  # ë³µí˜¸í™” í•¨ìˆ˜ ì„í¬íŠ¸
from config import (
    OPENAI_API_KEY,
    SUPABASE_URL,
    SUPABASE_KEY,
    LLM_MODEL,
    EMBEDDING_MODEL,
    EMBEDDING_DIMENSION,
    VECTORSTORE_PATH,
    VECTORSTORE_KEY_PATH,
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    SEPARATOR,
    LOADER_KWARGS,
    TEXT_SPLITTER_KWARGS,
    EMBEDDING_KWARGS,
    VECTOR_SEARCH_KWARGS,
    CONVERSATION_MEMORY_KWARGS,
    SYSTEM_PROMPT,
    USER_PROMPT_TEMPLATE,
    LLM_TEMPERATURE
)
from cryptography.fernet import Fernet
import asyncio
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

app = FastAPI()

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (ì‹¤ì œ ë°°í¬ ì‹œ íŠ¹ì • ë„ë©”ì¸ìœ¼ë¡œ ì œí•œ)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë‹ˆí„°ë§ ì„œë²„ ì‹œì‘
monitoring.start_metrics_server(port=8001)

# ì„¤ì • ë¡œë“œ
config = load_config()

try:
    # ë²¡í„°ìŠ¤í† ì–´ ë³µí˜¸í™” ì‹œë„
    if os.path.exists("faiss_vectorstore/index.enc"):
        print("ì•”í˜¸í™”ëœ ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì„ ë³µí˜¸í™”í•©ë‹ˆë‹¤.")
        decrypt_vectorstore("faiss_vectorstore", "vectorstore_key.key")
    
    # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
    vectorstore, _ = initialize_vectorstore(EMBEDDING_MODEL)
    print("ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ëŒ€í™” ê·¸ë˜í”„ ì´ˆê¸°í™” (ë²¡í„°ìŠ¤í† ì–´ ì „ë‹¬)
    conversation_graph = ConversationGraph(config, vectorstore)
except Exception as e:
    print(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    raise

# Supabase ì´ˆê¸°í™”
load_dotenv()

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("Supabase URL ë˜ëŠ” API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ì •ì  íŒŒì¼ ì„œë¹™ ì„¤ì •
app.mount("/static", StaticFiles(directory="static"), name="static")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# OpenAI ì„ë² ë”© ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(
    model=EMBEDDING_MODEL,
    openai_api_key=OPENAI_API_KEY
)

# FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”
dimension = 1536  # OpenAI ì„ë² ë”© ì°¨ì›
index = faiss.IndexFlatL2(dimension)

@app.post("/api/chat")
@monitoring.track_request("chat")
async def chat(chat_request: ChatRequest):
    try:
        # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í™•ì¸
        if not hasattr(conversation_graph, 'process_query'):
            logger.error("Conversation graph not properly initialized")
            raise HTTPException(status_code=500, detail="Conversation graph not properly initialized")
        
        # ëŒ€í™” ê·¸ë˜í”„ë¥¼ í†µí•´ ì²˜ë¦¬
        response = conversation_graph.process_query(chat_request.question, chat_request.user_id)
        
        # ì‘ë‹µ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
        if not response:
            logger.error("No response generated")
            raise HTTPException(status_code=500, detail="No response generated")
        
        return response
    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(e)}")

@app.post("/api/save-conversation")
async def save_conversation(chat_request: ChatRequest):
    try:
        # Supabaseì— ëŒ€í™” ê¸°ë¡ ì €ì¥
        data, count = supabase.table('conversations').insert({
            'user_id': chat_request.user_id,
            'question': chat_request.question,
            'response': chat_request.response,
            'created_at': datetime.now().isoformat()
        }).execute()
        
        # ì‚½ì…ëœ ë°ì´í„° í™•ì¸
        if not data:
            raise HTTPException(status_code=400, detail="Failed to save conversation")
            
        return {"status": "success", "data": data[1]}
    except Exception as e:
        error_handler.log_error({
            "timestamp": datetime.now().isoformat(),
            "error": {
                "message": str(e),
                "stack": traceback.format_exc()
            }
        })
        raise HTTPException(status_code=500, detail="Internal Server Error")

@app.get("/api/get-conversations/{user_id}")
async def get_conversations(user_id: str):
    try:
        # Supabaseì—ì„œ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ (ìµœì‹ ìˆœ ì •ë ¬)
        response = supabase.table('conversations')\
            .select('*')\
            .eq('user_id', user_id)\
            .order('created_at', desc=True)\
            .execute()
            
        if not response.data:
            return {"conversations": []}
            
        return {"conversations": response.data}
    except Exception as e:
        error_handler.log_error({
            "timestamp": datetime.now().isoformat(),
            "error": {
                "message": str(e),
                "stack": traceback.format_exc()
            }
        })
        raise HTTPException(status_code=500, detail="Internal Server Error")

@app.get("/")
async def get():
    return FileResponse("static/index.html")

# API ìƒíƒœ í™•ì¸ìš© ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
async def health_check():
    return {"status": "healthy"}

# Swagger UIëŠ” ìë™ìœ¼ë¡œ /docsì—ì„œ í™•ì¸ ê°€ëŠ¥

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    conversation_history = []
    
    try:
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ë³µí˜¸í™”/ì•”í˜¸í™” ìë™ ì²˜ë¦¬)
        vector_store = load_vectorstore(embeddings)
        if not vector_store:
            raise Exception("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨")
        print("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        await websocket.send_text(json.dumps({
            "type": "error",
            "message": f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
        }))
        return
    
    while True:
        data = await websocket.receive_text()
        json_data = json.loads(data)
        question = json_data.get("question")
        
        # ìœ ì € ì§ˆë¬¸ ì „ì†¡
        await websocket.send_text(json.dumps({
            "type": "user_message",
            "content": question
        }))
        
        try:
            # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            docs = vector_store.similarity_search(
                question, 
                k=VECTOR_SEARCH_KWARGS["k"],
                fetch_k=VECTOR_SEARCH_KWARGS["fetch_k"],
                score_threshold=VECTOR_SEARCH_KWARGS["score_threshold"]
            )
            
            if docs:
                context = "\n\n".join([doc.page_content for doc in docs])
                sources = [
                    f"ğŸ“š ì¶œì²˜: {doc.metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')}, "
                    f"ğŸ“„ í˜ì´ì§€: {doc.metadata.get('page', 'í˜ì´ì§€ ì •ë³´ ì—†ìŒ')}"
                    for doc in docs
                ]
                
                # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                prompt = USER_PROMPT_TEMPLATE.format(
                    conversation_history="\n".join([f"{'ì‚¬ìš©ì' if msg['role'] == 'user' else 'AI'}: {msg['content']}" for msg in conversation_history[-4:]]),
                    question=question,
                    context=context
                )

                # OpenAI API í˜¸ì¶œ
                response = client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[
                        {"role": "system", "content": SYSTEM_PROMPT},
                        *conversation_history[-4:],
                        {"role": "user", "content": prompt}
                    ],
                    temperature=LLM_TEMPERATURE,  # LLM_TEMPERATURE ì‚¬ìš©
                    stream=True
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ì „ì†¡
                ai_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        ai_response += content
                        await websocket.send_text(json.dumps({
                            "type": "ai_message",
                            "content": content
                        }))
                        await asyncio.sleep(0.01)
                
                # ì™„ì„±ëœ AI ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
                conversation_history.append({"role": "assistant", "content": ai_response})
                
                # ì¶œì²˜ ì •ë³´ ì „ì†¡
                await websocket.send_text(json.dumps({
                    "type": "ai_message",
                    "content": f"<br><br>ì°¸ê³ í•œ ë¬¸ì„œ:<br>" + "<br>".join(sources)
                }))
                
            else:
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "content": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }))
                
        except Exception as e:
            import traceback
            traceback.print_exc()
            await websocket.send_text(json.dumps({
                "type": "error",
                "content": f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }))

def generate_key(key_path):
    """ì•”í˜¸í™” í‚¤ ìƒì„± ë° ì €ì¥"""
    if not os.path.exists(key_path):
        key = Fernet.generate_key()
        with open(key_path, 'wb') as key_file:
            key_file.write(key)
    with open(key_path, 'rb') as key_file:
        return key_file.read()

def encrypt_vectorstore(vectorstore_path, key_path):
    """ë²¡í„°ìŠ¤í† ì–´ ì•”í˜¸í™”"""
    key = generate_key(key_path)
    f = Fernet(key)
    
    # index.faiss íŒŒì¼ ì•”í˜¸í™”
    faiss_path = os.path.join(vectorstore_path, "index.faiss")
    if os.path.exists(faiss_path):
        with open(faiss_path, 'rb') as file:
            encrypted_data = f.encrypt(file.read())
        with open(faiss_path + '.enc', 'wb') as file:
            file.write(encrypted_data)
        os.remove(faiss_path)
    
    # index.pkl íŒŒì¼ ì•”í˜¸í™”
    pkl_path = os.path.join(vectorstore_path, "index.pkl")
    if os.path.exists(pkl_path):
        with open(pkl_path, 'rb') as file:
            encrypted_data = f.encrypt(file.read())
        with open(pkl_path + '.enc', 'wb') as file:
            file.write(encrypted_data)
        os.remove(pkl_path)

def decrypt_vectorstore(vectorstore_path, key_path):
    """ë²¡í„°ìŠ¤í† ì–´ ë³µí˜¸í™”"""
    key = generate_key(key_path)
    f = Fernet(key)
    
    # index.faiss.enc íŒŒì¼ ë³µí˜¸í™”
    faiss_path = os.path.join(vectorstore_path, "index.faiss.enc")
    if os.path.exists(faiss_path):
        with open(faiss_path, 'rb') as file:
            decrypted_data = f.decrypt(file.read())
        with open(faiss_path[:-4], 'wb') as file:
            file.write(decrypted_data)
        os.remove(faiss_path)
    
    # index.pkl.enc íŒŒì¼ ë³µí˜¸í™”
    pkl_path = os.path.join(vectorstore_path, "index.pkl.enc")
    if os.path.exists(pkl_path):
        with open(pkl_path, 'rb') as file:
            decrypted_data = f.decrypt(file.read())
        with open(pkl_path[:-4], 'wb') as file:
            file.write(decrypted_data)
        os.remove(pkl_path)

def initialize_vectorstore(embedding_model):
    """ì„œë²„ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”"""
    try:
        embeddings = OpenAIEmbeddings(
            model=embedding_model, 
            openai_api_key=OPENAI_API_KEY
        )
        
        if os.path.exists(VECTORSTORE_PATH):
            vectorstore = load_vectorstore(embeddings)
        else:
            vectorstore = FAISS.from_texts([""], embeddings)
        
        return vectorstore, "hash_file_path"
    except Exception as e:
        logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        return None, None

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 