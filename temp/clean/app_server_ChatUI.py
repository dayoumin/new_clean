# app.py

import os
import sys
import openai
import logging
import warnings
import json
import re
import traceback
from pathlib import Path
from datetime import datetime
import faiss
import streamlit as st
from dotenv import load_dotenv
import tiktoken

from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS as LangChainFAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.base import BaseCallbackHandler
from langchain.docstore.document import Document

# =========================
# 1. CONFIG ë”•ì…”ë„ˆë¦¬ ì •ì˜
# =========================

CONFIG = {
    "model_name": "gpt-4o",  # LLM ëª¨ë¸ëª… (ì˜¤íƒ€ ìˆ˜ì •: "gpt-4o" â†’ "gpt-4")
    "embedding_model": "text-embedding-3-large",  # ì„ë² ë”© ëª¨ë¸ëª… (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ìˆ˜ì •)
    "vectorstore_path": "faiss_vectorstore",  # ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œ
    "hash_file_path": "faiss_vectorstore/document_hashes.json",  # ë¬¸ì„œ í•´ì‹œ íŒŒì¼ ê²½ë¡œ
    "api_keys": {
        "openai": None,  # OpenAI API í‚¤
    }
}

# =========================
# 2. ë¡œê¹… ì„¤ì •
# =========================

class CustomFormatter(logging.Formatter):
    def format(self, record):
        return f"\n--- {record.levelname} ---\n{super().format(record)}\n"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('chat_server.log', encoding='utf-8')
    ]
)

for handler in logging.root.handlers:
    handler.setFormatter(CustomFormatter())

logger = logging.getLogger(__name__)

warnings.filterwarnings("ignore", category=DeprecationWarning)

# =========================
# 3. API í‚¤ ë¡œë“œ
# =========================

load_dotenv()

CONFIG["api_keys"]["openai"] = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not CONFIG["api_keys"]["openai"]:
    logger.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

openai.api_key = CONFIG["api_keys"]["openai"]

# =========================
# 4. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜
# =========================

def extract_law_name(file_name):
    """
    íŒŒì¼ ì´ë¦„ì—ì„œ ë²•ë¥ ëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    name = Path(file_name).stem
    name = re.sub(r'[_\-]', ' ', name)
    return name

def load_document_hashes(hash_file_path: Path) -> dict:
    """ì €ì¥ëœ ë¬¸ì„œ í•´ì‹œ ë¡œë“œ"""
    try:
        return json.loads(hash_file_path.read_text())
    except Exception as e:
        logger.error(f"í•´ì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

# =========================
# 5. ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” í•¨ìˆ˜
# =========================

def initialize_vectorstore(embedding_model):
    VECTORSTORE_PATH = Path(CONFIG["vectorstore_path"])
    HASH_FILE_PATH = Path(CONFIG["hash_file_path"])

    embeddings = OpenAIEmbeddings(model=embedding_model, openai_api_key=CONFIG["api_keys"]["openai"])

    if 'vectorstore' in st.session_state:
        logger.info("ì„¸ì…˜ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state['vectorstore'], HASH_FILE_PATH

    try:
        vectorstore = LangChainFAISS.load_local(
            str(VECTORSTORE_PATH), 
            embeddings,
            allow_dangerous_deserialization=True
        )
        logger.info("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    st.session_state['vectorstore'] = vectorstore
    return vectorstore, HASH_FILE_PATH

# =========================
# 6. ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ì •ì˜
# =========================

class StreamlitCallbackHandler(BaseCallbackHandler):
    """Streamlitì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹µë³€ì„ í‘œì‹œí•˜ëŠ” ì½œë°± í•¸ë“¤ëŸ¬."""

    def __init__(self, message_placeholder):
        self.message_placeholder = message_placeholder
        self.answer_text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        """ìƒˆë¡œìš´ í† í°ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤."""
        skip_patterns = ["Human:", "Assistant:", "ì§ˆë¬¸: ", "ë‹µë³€: "]
        if any(pattern in token for pattern in skip_patterns):
            return

        self.answer_text += token
        self.message_placeholder.markdown(self.answer_text + "â–Œ")

    def on_llm_end(self, response, **kwargs):
        """LLM ì‘ë‹µ ì™„ë£Œë˜ë©´ í˜¸ì¶œë©ë‹ˆë‹¤."""
        final_answer = self.answer_text.strip()
        self.message_placeholder.markdown(final_answer)
        self.answer_text = ""

# =========================
# 7. í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
# =========================

def log_document_tokens(docs):
    """
    ê° ë¬¸ì„œì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë¡œê·¸ì— ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    total_tokens = 0
    for i, doc in enumerate(docs, 1):
        tokens = len(tokenizer.encode(doc.page_content))
        total_tokens += tokens
        logger.debug(f"[ë¬¸ì„œ {i}] í† í° ìˆ˜: {tokens}")

    logger.debug(f"ì´ ë¬¸ì„œ í† í° ìˆ˜: {total_tokens}")
    return total_tokens

# =========================
# 8. ChatPromptTemplate ì •ì˜
# =========================

chat_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """ë‹¹ì‹ ì€ í•œêµ­ì˜ ìµœê³  ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. êµ­ë¦½ìˆ˜ì‚°ê³¼í•™ì›ì˜ ê·¼ë¬´ìë“¤ì„ ìœ„í•´ ë…¸ë ¥í•˜ê³ , ë‹¤ìŒ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”:

            1. [ì¤‘ìš”] ë²•ë¥ ì  ì§ˆë¬¸ì€ 2~7ë²ˆ ì ˆì°¨ì— ë”°ë¼ ë‹µë³€ì„ í•˜ì„¸ìš”.

            2. [í•„ìˆ˜] ë‹µë³€ì€ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ë˜, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
            - ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€
            - ê´€ë ¨ ë²•ì  ê°œë… ì„¤ëª…
            - êµ¬ì²´ì ì¸ ë²•ì  ê·¼ê±°ì™€ í•´ì„
            - ì ìš© ê°€ëŠ¥í•œ ì¡°í•­ ì„¤ëª…
            - ë‹¨ì„œ ì¡°í•­, ì˜ˆì™¸ì‚¬í•­ ë“± íŠ¹ì´ì‚¬í•­ì€ ëª…í™•íˆ ì–¸ê¸‰

            3. ì°¸ê³  ë²•ë ¹ì„ ëª…ì‹œí• ë•ŒëŠ” ë¬¸ì„œì˜ ì œëª©ì— ìˆëŠ” ë²•ë ¹ê³¼ ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš”.         

            4. [ì¤‘ìš”] ì œê³µëœ ë¬¸ì„œì— ê°™ì€ ë‚´ìš©ì´ ì—¬ëŸ¬ ë¬¸ì„œì— ì–¸ê¸‰ëœ ê²½ìš°, ë²•ì  íš¨ë ¥ì— ë”°ë¼ ë‹¤ìŒ ìˆœì„œë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ë‘ê³  ë‹µë³€í•˜ì„¸ìš”:
            - ë²•ë¥ 
            - ì‹œí–‰ë ¹
            - ì—…ë¬´í¸ëŒ ë“± ë¹„ë²•ë ¹ ìë£Œ
            * ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ê´€ë ¨ ê·œì •ì´ ë°œê²¬ë  ê²½ìš°, ë²•ì  íš¨ë ¥ì´ ë†’ì€ ë¬¸ì„œë¥¼ ìš°ì„ ìœ¼ë¡œ ì¸ìš©í•˜ê³ , í•„ìš” ì‹œ ë‹¤ë¥¸ ìë£ŒëŠ” ë³´ì¶© ì„¤ëª…ì— í™œìš©í•˜ì„¸ìš”.             

            5. [í˜•ì‹] ë²•ë¥ ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”:
            âš–ï¸ ê²°ë¡ : í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬
            ğŸ“Œ ì°¸ê³  ë²•ë ¹: ì¸ìš©ëœ ë²•ë ¹ ëª©ë¡

            6. ë‹µë³€ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  
            ëª…ì‹œí•˜ê³  ê²°ë¡ , ì°¸ê³ ë²•ë ¹ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

            7. [ì°¸ê³ ] ëª¨ë“  ì¡°í•­ ì¸ìš© ì‹œ "ã€Œë²•ë¥ ëª…ã€ ì œXì¡° ì œXí•­"ê³¼ ê°™ì´ ì •í™•í•œ ì¶œì²˜ë¥¼ 
            í‘œì‹œí•˜ì„¸ìš”.
                    """
    ),
    ("user", "\n\n[ì œê³µëœ ë¬¸ì„œ]\n{context}\n\n[ì§ˆë¬¸]\n{question}")
])

# =========================
# 9. ë©”ì¸ í•¨ìˆ˜ ì •ì˜
# =========================

# í˜ì´ì§€ ì„¤ì •ì„ ë©”ì¸ í•¨ìˆ˜ ë°–ìœ¼ë¡œ ì´ë™
st.set_page_config(
        page_title=" ",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
        layout="wide",
        initial_sidebar_state="collapsed"
    )


# 3. í…Œë§ˆ ì„¤ì • ë° ìŠ¤íƒ€ì¼ ì ìš© (ë°”ë¡œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨)
current_hour = datetime.now().hour
is_dark_mode = current_hour < 6 or current_hour >= 18

if is_dark_mode:
    bg_color = "#1a1a1a"
    text_color = "#e0e0e0"
    header_bg = "linear-gradient(135deg, #2d2d2d, #1a1a1a)"
    chat_bg = "#2d2d2d"
    user_msg_bg = "#4a4a4a"
    assistant_msg_bg = "#333333"
    input_bg = "#2d2d2d"
    header_color = "#60a5fa"
else:
    bg_color = "#f5f6f7"
    text_color = "#1a1a1a"
    header_bg = "linear-gradient(135deg, #ffffff, #f8f9fa)"
    chat_bg = "#ffffff"
    user_msg_bg = "#FEE500"
    assistant_msg_bg = "#ffffff"
    input_bg = "#f8f9fa"
    header_color = "#1a73e8"

# CSS ìŠ¤íƒ€ì¼ ìˆ˜ì •
st.markdown("""
    <style>
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    #MainMenu {visibility: hidden;}
    header {display: none !important;}
    footer {visibility: hidden;}
    [data-testid="stToolbar"] {display: none !important;}
    
    /* ìƒë‹¨ í—¤ë” */
    .custom-header {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        height: 40px;
        background: #2d2d2d;
        display: flex;
        align-items: center;
        justify-content: center;
        z-index: 1000;
    }
    
    /* í—¤ë” ì œëª© */
    .header-title {
        font-size: 1.1rem;
        color: #ffffff;
        display: flex;
        align-items: center;
        gap: 8px;
    }
    
    /* ì±„íŒ… ì˜ì—­ ì¡°ì • */
    .stChatFloatingInputContainer {
        margin-top: 40px !important;
    }
    
    .stChatMessageContainer {
        padding-top: 40px !important;
    }
    </style>
    
    <div class="custom-header">
        <div class="header-title">
            <span>âš–ï¸</span>
            <span>ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡</span>
        </div>
    </div>
""", unsafe_allow_html=True)

def main():
    
    
    # ìŠ¤íƒ€ì¼ ì„¤ì • ì ìš©
    
    
    # ë²¡í„°ìŠ¤í† ì–´ì™€ í•´ì‹œ íŒŒì¼ ì´ˆê¸°í™”
    vectorstore, hash_file_path = initialize_vectorstore(CONFIG["embedding_model"])

    # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ì „ì—­)
    global tokenizer
    try:
        tokenizer = tiktoken.encoding_for_model(CONFIG["model_name"])
    except KeyError:
        tokenizer = tiktoken.get_encoding("cl100k_base")

    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” - ëŒ€í™” ê¸°ë¡ ìœ ì§€ë¥¼ ìœ„í•´ í•„ìš”
    if 'memory' not in st.session_state:
        st.session_state['memory'] = ConversationBufferMemory(
            memory_key="chat_history",
            human_prefix="ì§ˆë¬¸",
            ai_prefix="ë‹µë³€",
            return_messages=True,
            output_key='answer'
        )

    # QA ì²´ì¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìƒì„±)
    if 'qa_chain' not in st.session_state:
        llm = ChatOpenAI(
            model_name=CONFIG["model_name"],
            temperature=0,
            streaming=True,
            openai_api_key=CONFIG["api_keys"]["openai"]
        )

        st.session_state['qa_chain'] = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
            memory=st.session_state['memory'],
            return_source_documents=True,
            chain_type="stuff",
            combine_docs_chain_kwargs={
                "prompt": chat_prompt,
                "document_variable_name": "context",
            },
            verbose=False,
            output_key='answer'                
        )
        logger.info("QA ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ì„¸ì…˜ ìƒíƒœì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    else:
        callback_handler = st.session_state.get('callback_handler')

    # ë©”ì¸ ì»¨í…Œì´ë„ˆ ì‹œì‘
    st.markdown('<div class="main-container">', unsafe_allow_html=True)
    
    # í—¤ë” ë§ˆí¬ì—… ìˆ˜ì •
    st.markdown("""
        <div class="header">
            <h1 class="header-title">ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡</h1>
        </div>
    """, unsafe_allow_html=True)
    
    # ì±„íŒ… ì˜ì—­
    st.markdown('<div class="chat-area">', unsafe_allow_html=True)
    
    # ë©”ì‹œì§€ í‘œì‹œ
    if "messages" not in st.session_state:
        st.session_state.messages = []
        welcome_msg = {
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ì²­ë ´ë²•ë¥  ìƒë‹´ì±—ë´‡ì…ë‹ˆë‹¤. ë²•ë¥  ê´€ë ¨ ê¶ê¸ˆí•˜ì‹  ì ì„ ë¬¼ì–´ë³´ì„¸ìš”. ğŸ˜Š"
        }
        st.session_state.messages.append(welcome_msg)

    # ëª¨ë“  ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ì…ë ¥ì°½
    st.markdown('<div class="input-container">', unsafe_allow_html=True)
    if question := st.chat_input("ğŸ’­ ë²•ë¥  ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        try:
            # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = vectorstore.similarity_search(question, k=5)
            log_document_tokens(retrieved_docs)

            logger.info("\n=== ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
            logger.info(f"ì§ˆë¬¸: {question}")
            logger.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")

            # context ìƒì„±
            context = ""
            for i, doc in enumerate(retrieved_docs, 1):
                law_name = doc.metadata.get("law_name", "ì¶œì²˜ ì •ë³´ ì—†ìŒ")
                page = doc.metadata.get("page", "í˜ì´ì§€ ì •ë³´ ì—†ìŒ")
                content = doc.page_content

                context += f"\n\n ê´€ë ¨ë²•ë ¹ {i}] {law_name}, \nğŸ“„ í˜ì´ì§€ {page}: \nğŸ’¡ ë‚´ìš©:\n{content}\n"

                logger.info(f"\n[ë¬¸ì„œ {i}]")
                logger.info(f"ì¶œì²˜: {law_name}")
                logger.info(f"í˜ì´ì§€: {page}")
                logger.info(f"ë‚´ìš© ìš”ì•½: {content[:200]}...")

            logger.info("=== ê²€ìƒ‰ ê²°ê³¼ ì¢…ë£Œ ===\n")
            
            # ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                callback_handler = StreamlitCallbackHandler(message_placeholder)
                callback_manager = CallbackManager([callback_handler])
                
                # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì„ì‹œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                temp_llm = ChatOpenAI(
                    model_name=CONFIG["model_name"],
                    temperature=0,
                    streaming=True,
                    openai_api_key=CONFIG["api_keys"]["openai"],
                    callback_manager=callback_manager
                )
                
                # ì„ì‹œ QA ì²´ì¸ ìƒì„±
                temp_qa_chain = ConversationalRetrievalChain.from_llm(
                    llm=temp_llm,
                    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
                    memory=st.session_state['memory'],
                    return_source_documents=True,
                    chain_type="stuff",
                    combine_docs_chain_kwargs={
                        "prompt": chat_prompt,
                        "document_variable_name": "context",
                    },
                    verbose=False,
                    output_key='answer'                
                )
                
                response = temp_qa_chain({"question": question})
                
                # ì‘ë‹µì„ ë©”ì‹œì§€ ê¸°ë¡ì— ì¶”ê°€
                st.session_state.messages.append({"role": "assistant", "content": response['answer']})

            logger.info("=== ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ ===\n")

            # ì‘ë‹µ ìƒì„± í›„ ìŠ¤í¬ë¡¤
            st.markdown("""
                <script>
                    setTimeout(scrollToBottom, 100);
                    setTimeout(scrollToBottom, 500);
                </script>
            """, unsafe_allow_html=True)

        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {question}: {str(e)}")
            st.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            traceback.print_exc()
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()  # main() í•œ ë²ˆë§Œ í˜¸ì¶œ